Here is a step-by-step explanation of the E-Com Insight Pipeline project:

Data Generation

Objective: Develop a Python program to generate ~10,000 records of e-commerce data in CSV format. The records should follow the provided schema, simulating real-world transactions with rogue records for testing.
Key Fields: The data includes attributes such as order ID, customer name, product category, price, and payment details.
Exploratory Data Analysis (EDA) and Data Cleansing

Objective: Analyze the generated data to identify patterns, outliers, and inconsistencies.
Process:
Remove duplicates and irrelevant records.
Standardize formats for dates, numeric values, etc.
Handle rogue or incomplete records.
Output: Two CSV files â€“ the original raw data and the cleaned data are uploaded to Google Cloud Storage (GCS).
Data Upload

Objective: Implement the functionality in Python to upload the CSV files to GCS, ensuring data is securely stored for further use.
Data Loading into BigQuery

Objective: Load the cleansed CSV data into BigQuery. This step involves configuring BigQuery tables to follow the e-commerce schema, ensuring efficient querying and analysis.
Data Analysis

Objective: Perform SQL queries in BigQuery to address marketing questions, such as identifying top-selling product categories, peak sales times, and locations with the highest sales traffic.
Queries:
Top-selling categories by country.
Product trends over the year.
Highest traffic locations and sales times.
Data Visualization with Looker

Objective: Visualize the analyzed data using Looker to provide actionable insights. These visualizations will help the marketing team understand customer behavior and sales trends.
